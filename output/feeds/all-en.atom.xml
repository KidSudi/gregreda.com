<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Greg Reda</title><link href="http://www.gregreda.com/" rel="alternate"></link><link href="http://www.gregreda.com/feeds/all-en.atom.xml" rel="self"></link><id>http://www.gregreda.com/</id><updated>2013-06-03T00:00:00-05:00</updated><entry><title>Join vs Exists vs In (SQL)</title><link href="http://www.gregreda.com/2013/06/03/join-vs-exists-vs-in" rel="alternate"></link><updated>2013-06-03T00:00:00-05:00</updated><author><name>Greg Reda</name></author><id>tag:www.gregreda.com,2013-06-03:2013/06/03/join-vs-exists-vs-in</id><summary type="html">&lt;p&gt;Last weekend, I came across &lt;a href="http://en.wikipedia.org/wiki/Jeff_Atwood"&gt;Jeff Atwood&lt;/a&gt;'s excellent &lt;a href="http://www.codinghorror.com/blog/2007/10/a-visual-explanation-of-sql-joins.html"&gt;visual explanation of SQL joins&lt;/a&gt; on Hacker News.&lt;/p&gt;
&lt;p&gt;It reminded me of teaching SQL to the incoming batch of &lt;a href="http://www.pwc.com/us/en/forensic-services/technology-solutions.jhtml"&gt;PwC FTS&lt;/a&gt; associates a few years ago.  Not many of them had prior programming experience, much less SQL exposure, so it was a fun week to learn how well us instructors could teach the topic.&lt;/p&gt;
&lt;p&gt;Most of them intuitively picked up on how the IN clause worked, but struggled with EXISTS and JOINs initially.  An explanation that always seemed to help illustrate the concept was to show that often you can write the exact same query using an IN, EXISTS, or a JOIN.&lt;/p&gt;
&lt;p&gt;As an example, let's assume the following two tables, which we'll call &lt;em&gt;tableA&lt;/em&gt; and &lt;em&gt;tableB&lt;/em&gt;.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="n"&gt;id&lt;/span&gt;  &lt;span class="n"&gt;name&lt;/span&gt;    &lt;span class="n"&gt;id&lt;/span&gt;  &lt;span class="n"&gt;title&lt;/span&gt;
&lt;span class="o"&gt;--&lt;/span&gt;  &lt;span class="o"&gt;----&lt;/span&gt;    &lt;span class="o"&gt;--&lt;/span&gt;  &lt;span class="o"&gt;----&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="n"&gt;Kenny&lt;/span&gt;   &lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="n"&gt;Analyst&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="n"&gt;Rob&lt;/span&gt;     &lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="n"&gt;Sales&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt;   &lt;span class="n"&gt;Molly&lt;/span&gt;   &lt;span class="mi"&gt;3&lt;/span&gt;   &lt;span class="n"&gt;Manager&lt;/span&gt;
&lt;span class="mi"&gt;1&lt;/span&gt;   &lt;span class="n"&gt;Greg&lt;/span&gt;
&lt;span class="mi"&gt;2&lt;/span&gt;   &lt;span class="n"&gt;John&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we wanted to get everyone that's an Analyst, we could do the following:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;  &lt;span class="o"&gt;*&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;    &lt;span class="n"&gt;tableA&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;   &lt;span class="n"&gt;tableA&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt; &lt;span class="k"&gt;IN&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="n"&gt;tableB&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;tableB&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Analyst&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="c1"&gt;-- Returns 3 records - Kenny, Rob, and Greg&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For those not very familiar with SQL, this should be relatively easy to understand.  We have written a &lt;a href="http://en.wikipedia.org/wiki/Correlated_subquery"&gt;subquery&lt;/a&gt; that will get the &lt;em&gt;id&lt;/em&gt; for the &lt;em&gt;Analyst&lt;/em&gt; title in &lt;em&gt;tableB&lt;/em&gt;.  Using IN, we can then grab all of the employees from &lt;em&gt;tableA&lt;/em&gt; who have that title.&lt;/p&gt;
&lt;p&gt;While IN statements are fairly intuitive, they're often less efficient than the same query written as a JOIN or EXISTS statement would be.&lt;/p&gt;
&lt;p&gt;To produce the same results as above, we can do the following:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="c1"&gt;-- EXISTS&lt;/span&gt;
&lt;span class="k"&gt;SELECT&lt;/span&gt;  &lt;span class="o"&gt;*&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;    &lt;span class="n"&gt;tableA&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;   &lt;span class="k"&gt;EXISTS&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;tableB&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Analyst&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="n"&gt;tableA&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tableB&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="c1"&gt;-- JOIN (INNER is the default when only JOIN is specified)&lt;/span&gt;
&lt;span class="k"&gt;SELECT&lt;/span&gt;  &lt;span class="o"&gt;*&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;    &lt;span class="n"&gt;tableA&lt;/span&gt;
&lt;span class="k"&gt;JOIN&lt;/span&gt;    &lt;span class="n"&gt;tableB&lt;/span&gt;
    &lt;span class="k"&gt;ON&lt;/span&gt;  &lt;span class="n"&gt;tableA&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tableB&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;   &lt;span class="n"&gt;tableB&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Analyst&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In most cases, EXISTS or JOIN will be much more efficient (and faster) than an IN statement.  Why?&lt;/p&gt;
&lt;p&gt;When using an IN combined with a subquery, the database must process &lt;em&gt;the entire subquery&lt;/em&gt; first, then process the overall query as a whole, matching up based on the relationship specified for the IN.&lt;/p&gt;
&lt;p&gt;With an EXISTS or a JOIN, the database will return true/false while checking the relationship specified.  Unless the table in the subquery is &lt;em&gt;very&lt;/em&gt; small, EXISTS or JOIN will perform much better than IN.&lt;/p&gt;
&lt;p&gt;Furthermore, writing the query as a JOIN gives us some additional flexibility to easily return all of the employees if we'd like, or to even check for employees who do not have a title (orphan records).&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="c1"&gt;-- Return employees and display their title&lt;/span&gt;
&lt;span class="k"&gt;SELECT&lt;/span&gt;  &lt;span class="o"&gt;*&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;    &lt;span class="n"&gt;tableA&lt;/span&gt;
&lt;span class="k"&gt;JOIN&lt;/span&gt;    &lt;span class="n"&gt;tableB&lt;/span&gt;
    &lt;span class="k"&gt;ON&lt;/span&gt;  &lt;span class="n"&gt;tableA&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tableB&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="c1"&gt;-- 1 Kenny  1 Analyst&lt;/span&gt;
&lt;span class="c1"&gt;-- 1 Rob    1 Analyst&lt;/span&gt;
&lt;span class="c1"&gt;-- 1 Greg   1 Analyst&lt;/span&gt;
&lt;span class="c1"&gt;-- 2 John   2 Sales&lt;/span&gt;

&lt;span class="c1"&gt;-- Which employees do not have a title?&lt;/span&gt;
&lt;span class="k"&gt;SELECT&lt;/span&gt;  &lt;span class="o"&gt;*&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;    &lt;span class="n"&gt;tableA&lt;/span&gt;
&lt;span class="k"&gt;LEFT&lt;/span&gt; &lt;span class="k"&gt;JOIN&lt;/span&gt;   &lt;span class="n"&gt;tableB&lt;/span&gt;
    &lt;span class="k"&gt;ON&lt;/span&gt;  &lt;span class="n"&gt;tableA&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tableB&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;
&lt;span class="k"&gt;WHERE&lt;/span&gt;   &lt;span class="n"&gt;tableB&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt; &lt;span class="k"&gt;IS&lt;/span&gt; &lt;span class="k"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="c1"&gt;-- 4 Molly  NULL NULL&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the first query above, Molly falls out because she does not have a title.  If we would have liked her to appear in the record set, we could simply change the JOIN to a LEFT JOIN and she would appear with NULL data from &lt;em&gt;tableB&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;If you have many IN statements littered throughout your code, you should compare the performance of these queries against an EXISTS or JOIN version of the same query - you'll likely see performance gains.&lt;/p&gt;
&lt;p&gt;I hope this illustrated some of the subtle differences between INs, EXISTS, and JOINs.  Questions and feedback in the comments are appreciated.&lt;/p&gt;</summary><category term="sql"></category><category term="database"></category></entry><entry><title>More web scraping with Python (and a map)</title><link href="http://www.gregreda.com/2013/04/29/more-web-scraping-with-python" rel="alternate"></link><updated>2013-04-29T20:37:03-05:00</updated><author><name>Greg Reda</name></author><id>tag:www.gregreda.com,2013-04-29:2013/04/29/more-web-scraping-with-python</id><summary type="html">&lt;p&gt;&lt;em&gt;This is a follow-up to my &lt;a href="http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/" title="Web Scraping 101 with Python"&gt;previous post&lt;/a&gt; about web scraping with Python&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Previously, I wrote a basic intro to scraping data off of websites.  Since I wanted to keep the intro fairly simple, I didn't cover storing the data.  In this post, I'll cover the basics of writing the scraped data to a flat file and then take things a bit further from there.&lt;/p&gt;
&lt;p&gt;Last time, we used the Chicago Reader's Best of 2011 list, but let's change it up a bit this time and scrape a different site.  Why?  Because scrapers break, so we might as well practice a little bit more by scraping something different.&lt;/p&gt;
&lt;p&gt;In this post, we're going to use the data from &lt;a href="http://www.chicagomag.com/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/"&gt;Chicago Magazine's Best Sandwiches list&lt;/a&gt; because ... who doesn't like sandwiches?&lt;/p&gt;
&lt;p&gt;If you're new to scraping, it might be a good idea to go back and read my &lt;a href="http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/" title="Web Scraping 101 with Python"&gt;previous post&lt;/a&gt; as a refresher as I don't intend to be methodical in this one.&lt;/p&gt;
&lt;h4&gt;Finding the data&lt;/h4&gt;
&lt;p&gt;Looking at the list, it's clear everything is in a fairly standard format - each of the sandwiches in the list gets a &lt;em&gt;&lt;code&gt;&amp;lt;div class="sammy"&amp;gt;&lt;/code&gt;&lt;/em&gt; and each div holds a bit more information - specifically, the rank, sandwich name, location, and a URL to a detailed page about each entry.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Delicious sammy divs" src="/static/images/sammy-divs.png" /&gt;&lt;/p&gt;
&lt;p&gt;Clicking through a few of the sammy links, we can see that each sandwich also gets a detailed page that includes the sandwich's name, rank, description, and price along with the restaurant's name, address, phone number, and website.  Each of these details is contained within &lt;em&gt;&lt;code&gt;&amp;lt;div id="sandwich"&amp;gt;&lt;/code&gt;&lt;/em&gt;, which will make them very easy to get at.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sandwich details HTML" src="/static/images/sammy-details.png" /&gt;&lt;/p&gt;
&lt;h4&gt;Package choices&lt;/h4&gt;
&lt;p&gt;We'll again be using the &lt;a href="http://www.crummy.com/software/BeautifulSoup/"&gt;BeautifulSoup&lt;/a&gt; and &lt;a href="http://docs.python.org/2/library/urllib2.html"&gt;urllib2&lt;/a&gt; libraries.  Last time around, the choice of these two libraries generated some discussion in the post's &lt;a href="http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/#disqus_thread"&gt;comments section&lt;/a&gt;, on &lt;a href="http://www.reddit.com/r/Python/comments/19lnth/web_scraping_101_with_python_and_beautifulsoup/"&gt;Reddit&lt;/a&gt;, and &lt;a href="https://news.ycombinator.com/item?id=5353347"&gt;Hacker News&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The reason I use &lt;a href="http://www.crummy.com/software/BeautifulSoup/"&gt;BeautifulSoup&lt;/a&gt; is because I've found it to be very easy to use and understand, but YMMV.  It's been around for a very long time (since 2004) and is certainly in the tool belt of many.  That said, Python has a vast ecosystem with a lot of scraping libraries and ones like &lt;a href="http://scrapy.org/"&gt;Scrapy&lt;/a&gt; and &lt;a href="http://pythonhosted.org/pyquery/"&gt;PyQuery&lt;/a&gt; (amongst many others) are worth a look.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://docs.python.org/2/library/urllib2.html"&gt;Urllib2&lt;/a&gt; is &lt;em&gt;one&lt;/em&gt; of Python's URL handling packages within its standard library.  Because the standard library has &lt;a href="http://docs.python.org/2/library/urllib.html"&gt;urllib&lt;/a&gt; and &lt;a href="http://docs.python.org/2/library/urllib2.html"&gt;urllib2&lt;/a&gt;, it has at times been confusing to know which is the one you're actually looking for.  On top of that, &lt;a href="http://kennethreitz.org/"&gt;Kenneth Reitz&lt;/a&gt;'s fantastic &lt;a href="http://docs.python-requests.org/en/latest/"&gt;requests&lt;/a&gt; library exists, which really simplifies dealing with HTTP.&lt;/p&gt;
&lt;p&gt;In this example, and in the previous one, I use urllib2 simply because I &lt;em&gt;only&lt;/em&gt; need the &lt;a href="http://docs.python.org/2/library/urllib2.html#urllib2.urlopen"&gt;urlopen&lt;/a&gt; function.  If this scraper were more complex, I would likely use &lt;a href="http://docs.python-requests.org/en/latest/"&gt;requests&lt;/a&gt;, but I think using a third party library is a bit of overkill for this very simple use case.&lt;/p&gt;
&lt;h4&gt;Getting the data&lt;/h4&gt;
&lt;p&gt;Our code this time is going to be very similar to what it was &lt;a href="http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/" title="Web Scraping 101 with Python"&gt;in the previous post&lt;/a&gt;, save for a few minor changes.  Since the details pages have the data we're looking for, let's get all of their URLs from the initial list page, and then process each details page.  We're also going to write all of the data to a tab-delimited file using Python's &lt;a href="http://docs.python.org/2/library/csv.html"&gt;CSV&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;Last time around, we wrote our code as a set of functions, which I think helps the code's readability since it makes clear what each piece of the code is doing.  This time around, we're just going to write a short script since this is really a one-off thing - once we have our data written to a CSV, we don't really have a use for this code anymore.&lt;/p&gt;
&lt;p&gt;Our script will do the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Load our libraries&lt;/li&gt;
&lt;li&gt;Read our &lt;em&gt;base_url&lt;/em&gt; into a BeautifulSoup object, grab all &lt;em&gt;&lt;code&gt;&amp;lt;div class="sammy"&amp;gt;&lt;/code&gt;&lt;/em&gt; sections, and then from each section, grab our sammy details URL.&lt;/li&gt;
&lt;li&gt;Open up a file named &lt;em&gt;src-best-sandwiches.tsv&lt;/em&gt; for writing.  We'll write to this file using Python's &lt;a href="http://docs.python.org/2/library/csv.html#csv.writer"&gt;csv.writer&lt;/a&gt; object and separate the fields by a tab (\t).  We'll also pass in a list of field names so that our file has a header row.&lt;/li&gt;
&lt;li&gt;Loop through all of our sammy details URLs, grabbing each piece of information we're interested in, and writing that data to our &lt;em&gt;src-best-sandwiches.tsv&lt;/em&gt; file.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bs4&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;urllib2&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;urlopen&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;csv&lt;/span&gt;

&lt;span class="n"&gt;base_url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://www.chicagomag.com/Chicago-Magazine/&amp;quot;&lt;/span&gt;
            &lt;span class="s"&gt;&amp;quot;November-2012/Best-Sandwiches-Chicago/&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;soup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;urlopen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;base_url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;sammies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;div&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;sammy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;sammy_urls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;div&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;href&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;div&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sammies&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;data/src-best-sandwiches.tsv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;w&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;fieldnames&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;rank&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;sandwich&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;restaurant&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;description&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;price&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s"&gt;&amp;quot;address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;phone&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;website&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delimiter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writerow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fieldnames&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sammy_urls&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://www.chicagomag.com&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c"&gt;# inconsistent URL&lt;/span&gt;
        &lt;span class="n"&gt;page&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urlopen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://www.chicagomag.com{0}&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;soup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;page&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;div&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;sandwich&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
        &lt;span class="n"&gt;rank&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;div&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;id&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;sandRank&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode_contents&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;sandwich&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;h1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode_contents&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&amp;lt;br/&amp;gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;restaurant&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;h1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;span&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode_contents&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;description&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode_contents&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;addy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;p&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;addy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;em&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode_contents&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;addy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;partition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;address&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;addy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;partition&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;phone&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;p&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;addy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;em&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode_contents&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;p&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;addy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;em&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;website&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;p&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;addy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;em&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode_contents&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;website&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;

        &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writerow&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;rank&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sandwich&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;restaurant&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;address&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phone&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;website&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Done writing file&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;While our scraper does a good job of getting all of the sandwiches and restaurants, a couple of restaurants had "multiple locations" listed as their address.  If we were to need this data, we'll have to find another way to get it (like checking each restaurant's website and manually adding their locations to our dataset).  We'll also need to manually fix some oddities that wound up in our data due some inconsistent HTML on the other end (addresses and URLs winding up in the phone numbers column).&lt;/p&gt;
&lt;p&gt;We're now left with a file full of data about Chicago Magazine's fifty best sandwiches.  Sure, it's nice to have the data structured neatly in a flat file, but that's not all that interesting.&lt;/p&gt;
&lt;p&gt;Collecting and hoarding data isn't of use to anyone - it's a waste of a potentially very valuable resource - it needs to be taken a step further.  In some cases, this means a thorough analysis in search of patterns and trends, surfacing relationships we did not necessarily expect, and utilizing that information to better our decision-making.  Data should be used to inform.  In some cases, even a very basic visualization of the data can be of use.&lt;/p&gt;
&lt;p&gt;Since we have addresses for each restaurant, this seems like a great time to make a map, but first, geocoding!&lt;/p&gt;
&lt;h4&gt;Geocoding&lt;/h4&gt;
&lt;p&gt;We're going to make our map using the &lt;a href="https://developers.google.com/maps/"&gt;Google Maps API&lt;/a&gt;, but in order to do so, we're first going to need to geocode our addresses to a set of lat/long points.  Don't worry, I've taken the time to manually fill in the blanks on those "multiple locations" restaurants (you can grab the new file from my &lt;a href="https://github.com/gjreda/best-sandwiches"&gt;GitHub repo&lt;/a&gt; - it's called &lt;em&gt;best-sandwiches.tsv&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;To do so, we'll just write a short Python script which hits the &lt;a href="https://developers.google.com/maps/documentation/geocoding/"&gt;Google Geocoding API&lt;/a&gt;.  Our script will do the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Read our &lt;em&gt;best-sandwiches.tsv&lt;/em&gt; file using the CSV module's &lt;a href="http://docs.python.org/2/library/csv.html#csv.DictReader"&gt;DictReader&lt;/a&gt; class, which reads each line of the file into its own dictionary object.&lt;/li&gt;
&lt;li&gt;For each address, make a call to the Google Geocoding API, which will return a JSON response full of details about that address.&lt;/li&gt;
&lt;li&gt;Using the &lt;a href="http://docs.python.org/2/library/csv.html#csv.DictWriter"&gt;DictWriter&lt;/a&gt; class, write a new file with our data along with the formatted address, lat, and long that we got back from the geocoder.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;urllib2&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;urlopen&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;csv&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;time&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;sleep&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;geocode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;address&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;http://maps.googleapis.com/maps/api/geocode/json?&amp;quot;&lt;/span&gt;
        &lt;span class="s"&gt;&amp;quot;sensor=false&amp;amp;address={0}&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;address&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;+&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;urlopen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;data/best-sandwiches.tsv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;r&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;reader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DictReader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delimiter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;data/best-sandwiches-geocode.tsv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;w&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;fields&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;rank&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;sandwich&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;restaurant&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;description&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;price&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="s"&gt;&amp;quot;address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;city&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;phone&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;website&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;full_address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="s"&gt;&amp;quot;formatted_address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;lat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;lng&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;writer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DictWriter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fieldnames&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;fields&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;delimiter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writeheader&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;reader&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Geocoding: {0}&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;full_address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;geocode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;full_address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;status&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;u&amp;quot;OK&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;results&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;formatted_address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;formatted_address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;lat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;geometry&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;location&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;lat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;lng&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;geometry&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;location&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;lng&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;formatted_address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
                &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;lat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
                &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;lng&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
            &lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writerow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Done writing file&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Our file now has everything we need to make our map, which we're able to do with some basic HTML, CSS, JavaScript, and a little &lt;a href="https://developers.google.com/fusiontables/"&gt;Google Fusion Tables&lt;/a&gt; magic.&lt;/p&gt;
&lt;h4&gt;Mapping&lt;/h4&gt;
&lt;p&gt;While we could write another Python script to turn our flat file data into KML for mapping, it's much, much easier to use Google Fusion Tables.  However, one important note with the Fusion Tables approach is that the underlying data must be within a &lt;em&gt;public&lt;/em&gt; Fusion Table.  Since our data is scraped from a publicly accessible website, that's not an issue here.&lt;/p&gt;
&lt;p&gt;If you don't see Fusion Table as an option in your Google Drive account, you'll need to "connect more apps" and add it from there.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Adding Fusion Tables" src="/static/images/add-fusion-tables.png" /&gt;&lt;/p&gt;
&lt;p&gt;Once you've added the app, create a new Fusion Table from the delimited file on your computer (our &lt;em&gt;best-sandwiches-geocode.tsv&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Loading to Fusion Tables" src="/static/images/loading-to-fusion-tables.png" /&gt;&lt;/p&gt;
&lt;p&gt;After you've finished your upload process, you should now have a spreadsheet-like table with the data in it.  You'll notice that some of the columns are highlighted in yellow - this means that Fusion Tables is recognizing that it's a location.  Our lat and lng columns should be all the way at the right - hover over the lat column header and select &lt;em&gt;change&lt;/em&gt; from the drop down.  This should display a prompt showing us the column type is a two column location comprised of both our lat and lng.&lt;/p&gt;
&lt;p&gt;This is probably where I should point out that we could have also used Fusion Tables to geocode our data, but writing a script in Python seemed like more fun to me.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Lat Lng column type" src="/static/images/lat-lng-column-type.png" /&gt;&lt;/p&gt;
&lt;p&gt;Now that we have our data successfully in the Fusion Table, we can use a combination HTML, CSS, some JavaScript, and the Fusion Tables API to serve up a map (you could also just click the map tab in Fusion Tables to see an embedded map of the data, but that's not as fun).  We can even style the map with the &lt;a href="http://gmaps-samples-v3.googlecode.com/svn/trunk/styledmaps/wizard/index.html"&gt;Google Maps Style Wizard&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Head over to my &lt;a href="https://github.com/gjreda/best-sandwiches"&gt;GitHub repo&lt;/a&gt; to see the HTML, CSS, and JavaScript used to create the map (along with the rest of the code and data used throughout this post).  I've done my best to comment the &lt;em&gt;best-sandwiches.html&lt;/em&gt; file to indicate what each piece is doing.  I've also used HTML5's geolocation capabilities so that fellow Chicagoans can easily see which sandwiches are near them (it displays pretty nicely on a mobile browser, too).&lt;/p&gt;
&lt;p&gt;You can check out the awesome map we made &lt;a href="http://www.gregreda.com/best-sandwiches.html"&gt;here&lt;/a&gt;.  Note that if you aren't in Chicago and let your browser know your location, you likely won't see any of the data - you'll have to scroll over to Chicago.&lt;/p&gt;
&lt;p&gt;Hopefully you found this post fun and informative.  Was there something I didn't cover?  Let me know in the comments.&lt;/p&gt;</summary><category term="scraping"></category><category term="python"></category><category term="data"></category><category term="tutorial"></category><category term="maps"></category></entry><entry><title>Write online about what you love</title><link href="http://www.gregreda.com/2013/03/16/why-you-should-write-online" rel="alternate"></link><updated>2013-03-16T00:00:00-05:00</updated><author><name>Greg Reda</name></author><id>tag:www.gregreda.com,2013-03-16:2013/03/16/why-you-should-write-online</id><summary type="html">&lt;p&gt;The other week, I wrote a &lt;a href="http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/"&gt;very basic intro to web scraping with Python&lt;/a&gt;.  Some friends knew that I had experience scraping data and they wanted to learn, so I figured it would be a great opportunity to write something publicly and test how well I could explain it.&lt;/p&gt;
&lt;p&gt;I'll be extending that scraping post a bit more in the future, but first I wanted to write about how the week and a half since I posted it has gone - or, explain why I think you should write online about what you love.&lt;/p&gt;
&lt;h4&gt;How it started&lt;/h4&gt;
&lt;p&gt;Shortly after finishing the post and feeling fairly satisfied with the way it turned out, I posted it and e-mailed a link to three of my friends - two of which were the ones who asked me to teach them.  One of them, &lt;a href="https://twitter.com/kennylong"&gt;Kenny&lt;/a&gt;, immediately messaged me, read through the post, and said I should share it on Twitter.  &lt;a href="https://twitter.com/gjreda/status/308337050065727489"&gt;So I did&lt;/a&gt;.  To me, any feedback was better than no feedback, so I posted it &lt;a href="http://www.reddit.com/r/Python/comments/19lnth/web_scraping_101_with_python_and_beautifulsoup/"&gt;in the Python subreddit&lt;/a&gt; too.&lt;/p&gt;
&lt;p&gt;I was really just hoping some people would see it and let me know what they thought.&lt;/p&gt;
&lt;p&gt;Turns out, quite a few more people than my 92 followers (at the time) have seen it in the week and a half since.  About 32,000 more.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Initial traffic from Twitter and /r/python" src="/static/images/initial-traffic-20130313.png" /&gt;&lt;/p&gt;
&lt;p&gt;It was pretty exhilarating to watch something I wrote be shared in real-time.  Many of the &lt;a href="https://twitter.com/siah/status/308719789524799488"&gt;data&lt;/a&gt; &lt;a href="https://twitter.com/treycausey/status/308342790180458496"&gt;nerds&lt;/a&gt; that I admire and follow on Twitter were sharing it.  Hell, even Philadelphia's Chief Data Officer &lt;a href="https://twitter.com/mheadd/status/308576308810637312"&gt;shared it&lt;/a&gt;.  It was a ton of fun to watch and read both (fairly) positive and constructive comments about it on /r/python.  It immediately made me want to write the post you're currently reading, which I started working on two days later.&lt;/p&gt;
&lt;h4&gt;A week later&lt;/h4&gt;
&lt;p&gt;Sitting around the following Sunday, making minor CSS tweaks to this site and finishing up the previously mentioned post, I decided to check my Google Analytics to see what the final traffic from /r/python and Twitter looked like.  Surprisingly, the real-time section of Analytics showed 250+ on the site.  What?  How?!&lt;/p&gt;
&lt;p&gt;That's when I realized it wound up at the top of &lt;a href="https://news.ycombinator.com/item?id=5353347"&gt;Hacker News&lt;/a&gt;.  And then &lt;a href="http://www.reddit.com/r/programming/comments/1a20lf/web_scraping_101_with_python/"&gt;/r/programming&lt;/a&gt;.  Traffic went through the roof.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Hacker News, /r/programming, and lots of Twitter sharing" src="/static/images/more-traffic-20130313.png" /&gt;&lt;/p&gt;
&lt;p&gt;And again, the comments were positive and constructive.&lt;/p&gt;
&lt;h4&gt;Lesson learned&lt;/h4&gt;
&lt;p&gt;And this leads me to why you should write about things you're passionate about online.  When you're truly passionate about something, you spend a lot of time thinking and learning about it - you try to make it a part of your life.  You try to become reputable source on the topic (or even an expert).  It can be something as broad as beer, personal finance, or film; or as niche as stand-up comedy, vegan baking, or &lt;a href="http://en.wikipedia.org/wiki/Emo#Underground_popularity:_mid-1990s"&gt;90s midwest emo bands&lt;/a&gt; (guilty).  It doesn't matter what it is as long as &lt;em&gt;you&lt;/em&gt; love it.&lt;/p&gt;
&lt;p&gt;Like me, you're likely ecstatic when you find someone you're able to get nerdy with about something you love.  You truly enjoy the topic and are always looking for ways to &lt;em&gt;learn more&lt;/em&gt; and &lt;em&gt;teach others&lt;/em&gt; about it (or just banter).&lt;/p&gt;
&lt;p&gt;That's why you should share your knowledge about whatever the field may be.  &lt;em&gt;It doesn't matter whether 10 or 10,000 people see what you've shared&lt;/em&gt;.  There are people interested, but might not know where to start.  And that's the best way to reinforce how well we know something - by teaching it to others.  You'll be prompted with questions you hadn't thought about before, which will only further your own curiosity.  You're forced to explain concepts in simple terms that anyone can understand - you become a better teacher and communicator.  Sometimes, someone else crazily passionate about the same topic will even come along and teach you a thing or two.&lt;/p&gt;
&lt;p&gt;We all have a thirst for knowledge in some form.  The internet's a magnificent place to test our existing knowledge by teaching others and learning more throughout the process thanks to feedback from those with differing experiences.&lt;/p&gt;
&lt;p&gt;Put your passions out there.  More often than not, you'll be amazed at what you get back.&lt;/p&gt;</summary></entry><entry><title>Web Scraping 101 with Python</title><link href="http://www.gregreda.com/2013/03/03/web-scraping-101-with-python" rel="alternate"></link><updated>2013-03-03T00:00:00-06:00</updated><author><name>Greg Reda</name></author><id>tag:www.gregreda.com,2013-03-03:2013/03/03/web-scraping-101-with-python</id><summary type="html">&lt;p&gt;Yea, yea, I know I said I was going to &lt;a href="http://www.gregreda.com/2013/01/23/translating-sql-to-pandas-part1/"&gt;write more&lt;/a&gt; on &lt;a href="http://pandas.pydata.org"&gt;pandas&lt;/a&gt;, but recently I've had a couple friends ask me if I could teach them how to scrape data.  While they said they were able to find a ton of resources online, all assumed some level of knowledge already.  Here's my attempt at assuming a very minimal knowledge of programming.&lt;/p&gt;
&lt;h4&gt;Getting Setup&lt;/h4&gt;
&lt;p&gt;We're going to be using Python 2.7, &lt;a href="http://www.crummy.com/software/BeautifulSoup/"&gt;BeautifulSoup&lt;/a&gt;, and &lt;a href="http://lxml.de/"&gt;lxml&lt;/a&gt;.  If you don't already have Python 2.7, you'll want to download the proper version for your OS &lt;a href="http://python.org/download/releases/2.7.3/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To check if you have Python 2.7 on OSX, open up &lt;a href="http://en.wikipedia.org/wiki/Terminal_(OS_X)"&gt;Terminal&lt;/a&gt; and type &lt;em&gt;python --version&lt;/em&gt;.  You should see something like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="What Terminal should looks like" src="/static/images/python-version.png" /&gt;&lt;/p&gt;
&lt;p&gt;Next, you'll need to install &lt;a href="http://www.crummy.com/software/BeautifulSoup/"&gt;BeautifulSoup&lt;/a&gt;.  If you're on OSX, you'll already have &lt;a href="http://pypi.python.org/pypi/setuptools"&gt;setuptools&lt;/a&gt; installed.  Let's use it to install &lt;a href="http://www.pip-installer.org/en/latest/"&gt;pip&lt;/a&gt; and use that for package management instead.&lt;/p&gt;
&lt;p&gt;In Terminal, run &lt;em&gt;sudo easy_install pip&lt;/em&gt;.  You'll be prompted for your password - type it in and let it run.  Once that's done, again in Terminal, &lt;em&gt;sudo pip install BeautifulSoup4&lt;/em&gt;.  Finally, you'll need to &lt;a href="http://lxml.de/installation.html"&gt;install lxml&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;A few scraping rules&lt;/h4&gt;
&lt;p&gt;Now that we have the packages we need, we can start scraping.  But first, a couple of rules.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You should check a site's terms and conditions before you scrape them.  It's their data and they likely have some rules to govern it.&lt;/li&gt;
&lt;li&gt;Be nice - A computer will send web requests much quicker than a user can.  Make sure you space out your requests a bit so that you don't hammer the site's server.&lt;/li&gt;
&lt;li&gt;Scrapers break - Sites change their layout all the time.  If that happens, be prepared to rewrite your code.&lt;/li&gt;
&lt;li&gt;Web pages are inconsistent - There's sometimes some manual clean up that has to happen even after you've gotten your data.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Finding your data&lt;/h4&gt;
&lt;p&gt;For this example, we're going to use the &lt;a href="http://www.chicagoreader.com/chicago/best-of-chicago-2011/BestOf?oid=4100483"&gt;Chicago Reader's Best of 2011&lt;/a&gt; list.  Why?  Because I think it's a great example of terrible data presentation on the web.  Go ahead and browse it for a bit.&lt;/p&gt;
&lt;p&gt;All you want to see is a list of the category, winner, and maybe the runners-up, right?  But you have to continuously click link upon link, slowly navigating your way through the list.&lt;/p&gt;
&lt;p&gt;Hopefully in your clicking you noticed the important thing though - all the pages are structured the same.&lt;/p&gt;
&lt;h4&gt;Planning your code&lt;/h4&gt;
&lt;p&gt;In looking at the &lt;a href="http://www.chicagoreader.com/chicago/best-of-chicago-2011-food-drink/BestOf?oid=4106228"&gt;Food and Drink&lt;/a&gt; section of the Best of 2011 list, we see that all the categories are a link.  Each of those links has the winner, maybe some information about the winner (like an address), and the runners-up.  It's probably a good idea to break these things into separate functions in our code.&lt;/p&gt;
&lt;p&gt;To start, we need to take a look at the HTML that displays these categories.  If you're in Chrome or Firefox, highlight "Readers' Poll Winners", right-click, and select Inspect Element.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Inspect Element" src="/static/images/inspect-element.png" /&gt;&lt;/p&gt;
&lt;p&gt;This opens up the browser's Developer Tools (in Firefox, you might now have to click the HTML button on the right side of the developer pane to fully show it).  Now we'll be able to see the page layout.  The browser has brought us directly to the piece of HTML that's used to display the "Readers' Poll Winners" &lt;em&gt;&lt;code&gt;&amp;lt;dt&amp;gt;&lt;/code&gt;&lt;/em&gt; element.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Inspect Element some more" src="/static/images/inspect-element-more.png" /&gt;&lt;/p&gt;
&lt;p&gt;This seems to be the area of code where there's going to be some consistency in how the category links are displayed.  See that &lt;em&gt;&lt;code&gt;&amp;lt;dl class="boccat"&amp;gt;&lt;/code&gt;&lt;/em&gt; just above our "Readers' Poll Winners" line?  If you mouse over that line in your browser's dev tools, you'll notice that it highlights the &lt;strong&gt;entire section&lt;/strong&gt; of category links we want.  And every category link is within a &lt;em&gt;&lt;code&gt;&amp;lt;dd&amp;gt;&lt;/code&gt;&lt;/em&gt; element.  Perfect!  Let's get all of them.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Inspect Element mouse over" src="/static/images/inspect-element-mouseover.png" /&gt;&lt;/p&gt;
&lt;h4&gt;Our first function - getting the category links&lt;/h4&gt;
&lt;p&gt;Now that we know we know the &lt;em&gt;&lt;code&gt;&amp;lt;dl class="boccat"&amp;gt;&lt;/code&gt;&lt;/em&gt; section holds all the links we want, let's write some code to find that section, and then grab all of the links within the &lt;em&gt;&lt;code&gt;&amp;lt;dd&amp;gt;&lt;/code&gt;&lt;/em&gt; elements of that section.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bs4&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;urllib2&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;urlopen&lt;/span&gt;

&lt;span class="n"&gt;BASE_URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;http://www.chicagoreader.com&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_category_links&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;section_url&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;html&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urlopen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;section_url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;soup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;html&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;lxml&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;boccat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;dl&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;boccat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;category_links&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;BASE_URL&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;dd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;href&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;dd&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;boccat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findAll&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;dd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;category_links&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Hopefully this code is relatively easy to follow, but if not, here's what we're doing:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Loading the urlopen function from the urllib2 library into our local &lt;a href="http://en.wikipedia.org/wiki/Namespace_(computer_science)"&gt;namespace&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Loading the BeautifulSoup class from the bs4 (BeautifulSoup4) library into our local namespace.&lt;/li&gt;
&lt;li&gt;Setting a variable named &lt;em&gt;BASE_URL&lt;/em&gt; to "http://www.chicagoreader.com".  We do this because the links used through the site are relative - meaning they do not include the base domain.  In order to store our links properly, we need to concatenate the base domain with each relative link.&lt;/li&gt;
&lt;li&gt;Define a function named &lt;em&gt;get_category_links&lt;/em&gt;.&lt;ol&gt;
&lt;li&gt;The function requires a parameter of &lt;em&gt;section_url&lt;/em&gt;.  In this example, we're going to use the &lt;a href="http://www.chicagoreader.com/chicago/best-of-chicago-2011-food-drink/BestOf?oid=4106228"&gt;Food and Drink&lt;/a&gt; section of the BOC list, however we could use a different section URL - for instance, the &lt;a href="http://www.chicagoreader.com/chicago/best-of-chicago-2011-city-life/BestOf?oid=4106233"&gt;City Life&lt;/a&gt; section's URL.  We're able to create just one generic function because each section page is structured the same.&lt;/li&gt;
&lt;li&gt;Open the section_url and read it in the &lt;em&gt;html&lt;/em&gt; object.&lt;/li&gt;
&lt;li&gt;Create an object called &lt;em&gt;soup&lt;/em&gt; based on the BeautifulSoup class.  The &lt;em&gt;soup&lt;/em&gt; object is an &lt;a href="http://en.wikipedia.org/wiki/Instance_(computer_science)"&gt;instance&lt;/a&gt; of the BeautifulSoup class.  It is initialized with the html object and parsed with &lt;a href="http://lxml.de/"&gt;lxml&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;In our BeautifulSoup instance (which we called &lt;em&gt;soup&lt;/em&gt;), find the &lt;em&gt;&lt;code&gt;&amp;lt;dl&amp;gt;&lt;/code&gt;&lt;/em&gt; element with a class of "boccat" and store that section in a variable called &lt;em&gt;boccat&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;This is a &lt;a href="http://docs.python.org/2/tutorial/datastructures.html#list-comprehensions"&gt;list comprehension&lt;/a&gt;.  For every &lt;em&gt;&lt;code&gt;&amp;lt;dd&amp;gt;&lt;/code&gt;&lt;/em&gt; element found within our &lt;em&gt;boccat&lt;/em&gt; variable, we're getting the href of its &lt;em&gt;&lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt;&lt;/em&gt; element (our category links) and concatenating on our &lt;em&gt;BASE_URL&lt;/em&gt; to make it a complete link.  All of these links are being stored in a list called &lt;em&gt;category_links&lt;/em&gt;.  You could also write this line with a &lt;a href="http://docs.python.org/2/tutorial/controlflow.html#for-statements"&gt;for loop&lt;/a&gt;, but I prefer a list comprehension here because of its simplicity.&lt;/li&gt;
&lt;li&gt;Finally, our function returns the &lt;em&gt;category_links&lt;/em&gt; list that we created on the previous line.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Our second function - getting the category, winner, and runners-up&lt;/h4&gt;
&lt;p&gt;Now that we have our list of category links, we'd better start going through them to get our winners and runners-up.  Let's figure out which elements contain the parts we care about.&lt;/p&gt;
&lt;p&gt;If we look at the &lt;a href="http://www.chicagoreader.com/chicago/best-chef/BestOf?oid=4088191"&gt;Best Chef&lt;/a&gt; category, we can see that our category is in &lt;em&gt;&lt;code&gt;&amp;lt;h1 class="headline"&amp;gt;&lt;/code&gt;&lt;/em&gt;.  Shortly after that, we find our winner and runners-up stored in &lt;em&gt;&lt;code&gt;&amp;lt;h2 class="boc1"&amp;gt;&lt;/code&gt;&lt;/em&gt; and &lt;em&gt;&lt;code&gt;&amp;lt;h2 class="boc2"&amp;gt;&lt;/code&gt;&lt;/em&gt;, respectively.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Finding our winners and runners-up" src="/static/images/winners-and-runners-up.png" /&gt;&lt;/p&gt;
&lt;p&gt;Let's write some code to get all of them.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_category_winner&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;category_url&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;html&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urlopen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;category_url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;soup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;html&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;lxml&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;category&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;h1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;headline&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;
    &lt;span class="n"&gt;winner&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;h2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;h2&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findAll&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;h2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;boc1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="n"&gt;runners_up&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;h2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;h2&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findAll&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;h2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;boc2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;category&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;category&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s"&gt;&amp;quot;category_url&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;category_url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s"&gt;&amp;quot;winner&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;winner&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s"&gt;&amp;quot;runners_up&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;runners_up&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It's very similar to our last function, but let's walk through it anyway.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Define a function called &lt;em&gt;get_category_winner&lt;/em&gt;.  It requires a &lt;em&gt;category_url&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Lines two and three are actually exactly the same as before - we'll come back to this in the next section.&lt;/li&gt;
&lt;li&gt;Find the string within the &lt;em&gt;&lt;code&gt;&amp;lt;h1 class="headline"&amp;gt;&lt;/code&gt;&lt;/em&gt; element and store it in a variable named category.&lt;/li&gt;
&lt;li&gt;Another list comprehension - store the string within every &lt;em&gt;&lt;code&gt;&amp;lt;h2 class="boc1"&amp;gt;&lt;/code&gt;&lt;/em&gt; element in a list called &lt;em&gt;winner&lt;/em&gt;.  But shouldn't there be only one winner?  You'd think that, but some have multiple (e.g. &lt;a href="http://www.chicagoreader.com/chicago/best-bang-for-your-buck/BestOf?oid=4088018"&gt;Best Bang for your Buck&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Same as the previous line, but this time we're getting the runners-up.&lt;/li&gt;
&lt;li&gt;Finally, return a &lt;a href="http://docs.python.org/2/tutorial/datastructures.html#dictionaries"&gt;dictionary&lt;/a&gt; with our data.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;DRY - Don't Repeat Yourself&lt;/h4&gt;
&lt;p&gt;As mentioned in the previous section, lines two and three of our second function mirror lines in our first function.&lt;/p&gt;
&lt;p&gt;Imagine a scenario where we want to change the parser we're passing into our BeautifulSoup instance (in this case, lxml).  With the way we've currently written our code, we'd have to make that change in two places.  Now imagine you've written many more functions to scrape this data - maybe one to get addresses and another to get &lt;a href="http://www.chicagoreader.com/chicago/best-new-food-truckfood/BestOf?oid=4101387"&gt;paragraphs of text about the winner&lt;/a&gt; - you've likely repeated those same two lines of code in these functions and you now have to remember to make changes in four different places.  That's not ideal.&lt;/p&gt;
&lt;p&gt;A good principle in writing code is &lt;a href="http://en.wikipedia.org/wiki/Don't_repeat_yourself"&gt;DRY - Don't Repeat Yourself&lt;/a&gt;.  When you notice that you've written the same lines of code a couple times throughout your script, it's probably a good idea to step back and think if there's a better way to structure that piece.&lt;/p&gt;
&lt;p&gt;In our case, we're going to write another function to simply process a URL and return a BeautifulSoup instance.  We can then call this function in our other functions instead of duplicating our logic.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;make_soup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;html&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urlopen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;html&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;lxml&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We'll have to change our other functions a bit now, but it's pretty minor - we just need to replace our duplicated lines with the following:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="n"&gt;soup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;make_soup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c"&gt;# where url is the url we&amp;#39;re passing into the original function&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Putting it all together&lt;/h4&gt;
&lt;p&gt;Now that we have our main functions written, we can write a script to output the data however we'd like.  Want to write to a CSV file?  Check out Python's &lt;a href="http://docs.python.org/2/library/csv.html#csv.DictWriter"&gt;DictWriter&lt;/a&gt; class.  Storing the data in a database?  Check out the &lt;a href="http://docs.python.org/2/library/sqlite3.html"&gt;sqlite3&lt;/a&gt; or &lt;a href="http://wiki.python.org/moin/DatabaseInterfaces"&gt;other various database libraries&lt;/a&gt;.  While both tasks are somewhat outside of my intentions for this post, if there's interest, let me know in the comments and I'd be happy to write more.&lt;/p&gt;
&lt;p&gt;Hopefully you found this post useful.  I've put a final example script in &lt;a href="http://bit.ly/13yd9ng"&gt;this gist&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Read my follow-up to this post &lt;a href="http://www.gregreda.com/2013/04/29/more-web-scraping-with-python/"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;</summary><category term="scraping"></category><category term="python"></category><category term="data"></category><category term="tutorial"></category></entry><entry><title>Translating SQL to Pandas, Part 1</title><link href="http://www.gregreda.com/2013/01/23/translating-sql-to-pandas-part1" rel="alternate"></link><updated>2013-01-23T00:00:00-06:00</updated><author><name>Greg Reda</name></author><id>tag:www.gregreda.com,2013-01-23:2013/01/23/translating-sql-to-pandas-part1</id><summary type="html">&lt;p&gt;For some reason, I've always found SQL to a much more intuitive tool for exploring a tabular dataset than I have other languages (namely R and Python).&lt;/p&gt;
&lt;p&gt;If you know SQL well, you can do a whole lot with it, and since data is often in a relational database anyway, it usually makes sense to stick with it.  I find that my workflow often includes writing a lot of queries in SQL (using &lt;a href="http://www.sequelpro.com/"&gt;Sequel Pro&lt;/a&gt;) to get the data the way I want it, reading it into R (with &lt;a href="http://www.rstudio.com/"&gt;RStudio&lt;/a&gt;), and then maybe a bit more exploration, modeling, and visualization (with &lt;a href="http://ggplot2.org/"&gt;ggplot2&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Not too long ago though, I came across &lt;a href="http://blog.wesmckinney.com/"&gt;Wes McKinney&lt;/a&gt;'s &lt;a href="http://pandas.pydata.org"&gt;pandas&lt;/a&gt; package and my interest was immediately piqued.  Pandas adds a bunch of functionality to Python, but most importantly, it allows for a DataFrame data structure - much like a database table or R's data frame.&lt;/p&gt;
&lt;p&gt;Given the great things I've been reading about pandas lately, I wanted to make a conscious effort to play around with it.  Instead of my typical workflow being a couple disjointed steps with SQL + R + (sometimes) Python, my thought is that it might make sense to have pandas work its way in and take over the R work.  While I probably won't be able to completely give up R (too much ggplot2 love over here), I get bored if I'm not learning something new, so pandas it is.&lt;/p&gt;
&lt;p&gt;I intend to document the process a bit - hopefully a couple posts illustrating the differences between SQL and pandas (and maybe some R too).&lt;/p&gt;
&lt;p&gt;Throughout the rest of this post, we're going to be working with data from the &lt;a href="https://data.cityofchicago.org"&gt;City of Chicago's open data&lt;/a&gt; - specifically the &lt;a href="https://data.cityofchicago.org/Transportation/Towed-Vehicles/ygr5-vcbg"&gt;Towed Vechicles data&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Loading the data&lt;/h4&gt;
&lt;h5&gt;Using SQLite&lt;/h5&gt;
&lt;p&gt;To be able to use SQL with this dataset, we'd first have to create the table.  Using &lt;a href="http://www.sqlite.org/"&gt;SQLite&lt;/a&gt; syntax, we'd run the following:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;towed&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;tow_date&lt;/span&gt; &lt;span class="nb"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;make&lt;/span&gt; &lt;span class="nb"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;style&lt;/span&gt; &lt;span class="nb"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="nb"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="nb"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;plate&lt;/span&gt; &lt;span class="nb"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="k"&gt;state&lt;/span&gt; &lt;span class="nb"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;towed_address&lt;/span&gt; &lt;span class="nb"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;phone&lt;/span&gt; &lt;span class="nb"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;inventory&lt;/span&gt; &lt;span class="nb"&gt;text&lt;/span&gt;
&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Because SQLite &lt;a href="http://www.sqlite.org/datatype3.html"&gt;uses a very generic type system&lt;/a&gt;, we don't get the strict data types that we would in most other databases (such as MySQL and PostgreSQL); therefore, all of our data is going to be stored as text.  In other databases, we'd store tow_date as a date or datetime field.&lt;/p&gt;
&lt;p&gt;Before we read the data into SQLite, we need to tell the database to that the fields are separated by a comma.  Then we can use the import command to read the file into our table.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;separator&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;
&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Towed_Vehicles&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;csv&lt;/span&gt; &lt;span class="n"&gt;towed&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that the downloaded CSV contains two header rows, so we'll need to delete those from our table since we don't need them.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="k"&gt;DELETE&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;towed&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;tow_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Tow Date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We should have 5,068 records in our table now (note: the City of Chicago regularly updates this dataset, so you might get a different number).&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="k"&gt;COUNT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;-- 5068&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;Using Python + pandas&lt;/h5&gt;
&lt;p&gt;Let do the same with &lt;a href="http://pandas.pydata.org"&gt;pandas&lt;/a&gt; now.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;col_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;tow_date&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;make&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;style&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;model&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;color&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;plate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;state&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;towed_address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;phone&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;inventory&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;towed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Towed_Vehicles.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;col_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;skiprows&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parse_dates&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;tow_date&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The read_csv function in pandas actually allowed us to skip the two header columns and translate the tow_date field to a datetime field.&lt;/p&gt;
&lt;p&gt;Let's check our count just to make sure.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c"&gt;# 5068&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Selecting data&lt;/h4&gt;
&lt;h5&gt;SQL&lt;/h5&gt;
&lt;p&gt;Selection data with SQL is fairly intuitive - just SELECT the columns you want FROM the particular table you're interested in.  You can also take advantage of the LIMIT clause to only see a subset of your data.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="c1"&gt;-- Return every column for every record in the towed table&lt;/span&gt;
&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="c1"&gt;-- Return the tow_date, make, style, model, and color for every record in the towed table&lt;/span&gt;
&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="n"&gt;tow_date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;make&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="c1"&gt;-- Return every column for the first five records of the towed table&lt;/span&gt;
&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;towed&lt;/span&gt; &lt;span class="k"&gt;LIMIT&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="c1"&gt;-- Return every column in the towed table - start at the fifth record and show the next ten&lt;/span&gt;
&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;towed&lt;/span&gt; &lt;span class="k"&gt;LIMIT&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;-- records 5-14&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Additionally, you can throw a WHERE or ORDER BY (or both) into your queries for proper filtering and ordering of the data returned:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;towed&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="k"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;TX&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;-- Only towed vehicles from Texas&lt;/span&gt;

&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;towed&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;make&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;KIA&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;AND&lt;/span&gt; &lt;span class="k"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;TX&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;-- KIAs with Texas plates&lt;/span&gt;

&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;towed&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;make&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;KIA&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;ORDER&lt;/span&gt; &lt;span class="k"&gt;BY&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;-- All KIAs ordered by color (A to Z)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;Python + pandas&lt;/h5&gt;
&lt;p&gt;Let's do some of the same, but this time let's use pandas:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="c"&gt;# show only the make column for all records&lt;/span&gt;
&lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;make&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c"&gt;# tow_date, make, style, model, and color for the first ten records&lt;/span&gt;
&lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;tow_date&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;make&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;style&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;model&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;color&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]][:&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c"&gt;# first five rows (alternatively, you could use towed.head())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Because pandas is built on top of &lt;a href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt;, we're able to use &lt;a href="http://pandas.pydata.org/pandas-docs/dev/indexing.html#boolean-indexing"&gt;boolean indexing&lt;/a&gt;.  Since we're going to replicate similar statements to the ones we did in SQL, we know we're going to need towed cars from TX made by KIA.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;state&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;TX&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="c"&gt;# all columns and records where the car was from TX&lt;/span&gt;

&lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;state&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;TX&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;make&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;KIA&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="c"&gt;# made by KIA AND from TX&lt;/span&gt;

&lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;state&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;MA&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;make&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;JAGU&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="c"&gt;# made by Jaguar OR from MA&lt;/span&gt;

&lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;towed&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;make&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;KIA&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;color&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c"&gt;# made by KIA, ordered by color (A to Z)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h5&gt;Conclusion, Part 1&lt;/h5&gt;
&lt;p&gt;This was obviously a very basic start, but there are a lot of good things about pandas - it's certainly concise and readable.  Plus, since it works well with the various science + math packages (&lt;a href="http://www.scipy.org"&gt;SciPy&lt;/a&gt;, &lt;a href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt;, &lt;a href="http://matplotlib.org/"&gt;Matplotlib&lt;/a&gt;, &lt;a href="http://statsmodels.sourceforge.net/"&gt;statsmodels&lt;/a&gt;, etc.), there's the potential to work almost entirely in one language for analysis tasks.&lt;/p&gt;
&lt;p&gt;I plan on covering aggregate functions, pivots, and maybe some matplotlib in my next post.&lt;/p&gt;</summary><category term="sql"></category><category term="python"></category><category term="pandas"></category></entry><entry><title>Hello World</title><link href="http://www.gregreda.com/2013/01/22/hello-world" rel="alternate"></link><updated>2013-01-22T00:00:00-06:00</updated><author><name>Greg Reda</name></author><id>tag:www.gregreda.com,2013-01-22:2013/01/22/hello-world</id><summary type="html">&lt;p&gt;So I finally got around to putting something of my own up.&lt;/p&gt;
&lt;p&gt;My intentions are mainly to use this space as a way to document mini projects that I'm working on, so plan on it being pretty programming, data, visualization, and statistics heavy - I get bored if I'm not learning something new or doing something I find challenging.  That said, I'm known to go on tangents about music and beer (and whatever else I feel like ranting about at the time).&lt;/p&gt;
&lt;p&gt;There's also a high likelihood that I'll constantly be tweaking the layout of the site - hopefully the four people reading won't mind.&lt;/p&gt;</summary></entry></feed>